# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v83QnQLuzCBgzn9jQZWtOyIz9CaM_7PM
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

#create dummy data for training
x_values= [i for i in range(11)]
x_train= np.array(x_values,dtype=np.float32)
x_train=x_train.reshape(-1,1)

y_values=[2*i+1 for i in x_values]
y_train= np.array(y_values,dtype=np.float32)
y_train= y_train.reshape(-1,1)
y_train

x_train.shape

#create class
class LinearRegressinModel(nn.Module):
  def __init__(self,input_dim, output_dim):
    super(LinearRegressinModel,self).__init__()
    self.linear=nn.Linear(input_dim, output_dim)

def forward(self,x):
  out=self.Linear(x)
  return out

input_dim=1
output_dim=1

model= LinearRegressinModel(input_dim,output_dim)

linear=nn.Linear(2,3)
print(linear.weight)
print(linear.bias)

criterion= nn.MSELoss()

learning_rate=0.01
optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)

epochs=100
for epoch in range(epochs):
  #changing our variables to torch variables
  inputs=torch.from_numpy(x_train).requires_grad_()
  labels= torch.from_numpy(y_train)
  #clear gradients
  optimizer.zero_grad()

  #forward to get outputs
  outputs=model(inputs)

  #calculate loss
  Loss= criterion(outputs, labels)

  #getting gradients
  Loss.backward()

  #updating parameters
  optimizer.step()

  print('epoch{},loss{}'.format(epoch,loss.item()))